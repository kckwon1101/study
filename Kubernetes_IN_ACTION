kubectl get nodes
kubectl describe node gke-kubia-default-pool-1340aba3-zpv1
kubectl run kubia --image=luksa/kubia --port=8080 --generator=run/v1
kubectl get pods
kubectl expose rc kubia --type=LoadBalancer --name kubia-http
kubectl get services
curl 34.85.69.168:8080
kubectl get replicationcontrollers
kubectl get rc
kubectl get pods
kubectl get pods -o wide
kubectl describe pod kubia-5ps8b
사이드카 컨테이너의 종류에는 로그 로테이션 및 수집 장치, 데이터 프로세서, 통신 어댑터 등
kubectl get po kubia-5ps8b -o yaml
포드 디스크립터에 포트를 명시적으로 정의하면 각 포트에 이름을 할당할 수도 있으므로 유용
컨테이너화된 애플리케이션은 대게 로그를 파일에 쓰는 대신, 표준 출력과 표준 오류 스트림에 기록한다
docker logs <container id>
kubectl logs kubia-manual
kubia-manual = <pod name>
컨테이너 로그는 매일 로그 파일이 10MB 크기에 도달할 때마다 자동으로 교체된다. kubectl  logs 명령은 마지막 교체의 로그 항목만 표시한다.
kubectl logs kubia-manual -c <container name> 도 가능 : 컨테이너의 로그


 - 포드의 포트에 로컬 네트워크 포트 포워딩
 kubectl port-forward <pod name> <local port>:<pod port>
 

3.3 라벨을 이용한 포드 구성
  3.3.1 라벨 소개
    라벨 : 리소스에 첨부하는 임의의 키/값 쌍
 
  3.3.2 포드를 만들 때 라벨 지정하기
   라벨 셀렉터를 사용해 리소스를 선택할 때 활용된다.(리소스를 다시 만들지 않고도 기존 라벨의 값 수정 가능)
   디스크립션의 metadata.labes 아래에 작성
   
   kubectl get po --show-labels
   kubectl get po -L <label name> : 특정 라벨만 보기(<label name> ,를 이용해 여러개 장석 가능)
  
  3.3.3 포드의 라벨 수정
   라벨 추가 : kubectl label po <pod name> <key>=<value>
   라벨 수정 : kubectl label po <pod name> <key>=<valeu> --overwrite
  
3.4 라벨셀렉터를 통한 하위 집합 나열하기
 1. 특정 키가 있는 라벨 포함(또는 포함하지 않음)
 2. 특정 키와 값이 있는 라벨을 포함
 3. 특정 키가 있집만 지정한 값과 다른 값이 있는 라벨을 포함
 
  3.4.1 라벨 셀렉터를 사용한 포드 나열
   kubectl get po -l <key>=<value>
   kubectl get po -l <key>=<value>, <key>=<value> : 다중 조건
   kubectl get po -l <pod name>
   kubectl get po -l '!<pod name>' : bash 쉘이 느낌표를 인식하지 못하도록 pod name 사이에 따옴표 넣기
   <key>!=<valeu>
   <key> in (<pod name>, <pod name>)
   <key> notin (<pod name>, <pod name>)


3.5 포드 스케줄링 제약을 위한 라벨과 셀렉터의 사용
 라벨은 노드를 포함한 모든 쿠버네티스 객체에 붙일 수 있다.

  3.5.1 라벨을 사용한 워커 노드 분류
   kubectl label node <node name> <key>=<value> : 노드에 라벨 추가
   
  3.5.2 노드 지정을 위한 포드 스케줄링
   디스크립션의 spec.nodeSelector 아래에 <key>: "<value>" 추가 ( *큰 따옴표는 왜 여기에만 들어가는가???)
   > 포드를 만들면 스케줄러는 특정 라벨이 있는 노드 중에서 선택하여 생성
   ( *노드가 오프라인인 경우 문제 발생 > 항상라벨 셀렉터를 통해 지정된 특정 기준을 충족하는 노드의 논리적인 그룹을 생각해야 한다.
     지금은 그저 예제고 16장에서 자세히 다룬다.)


3.6 포드에 주석 달기
  3.6.1 객체의 주석 찾아보기
   주석은 디스크립션의 metadata.annotations의 아래에 <key>: <value>
   이 때, value에 JSON 데이터로 넣어도 되고 짧은 string도 가능. (256kb)
  
  3.6.2 주석 추가와 수정
   주석 추가 : kubectl annotate pod <pod name> <key>:"<value>"


3.7 그룹 리소스의 네임스페이스 사용하기
  3.7.1 네임프세이스의 필요성
    여러 네임스페이스를 사용하면 많은 구성 요소를 포함하는 복잡한 시스템을 더 작은 그룹으로 분할할 수 있다. 또한 멀티 테넌트 환경에서 리소스를
   분리하고 리소스를 생산, 개발 및 QA 환경으로 또는 기타 필요한 방ㅅ힉으로 분할하는 데 사용할 수 있다. 리소스 이름은 네임스페이스 내에서만
   고유해야 한다.
    대부분의 리소스 유형은 네임스페이스지만 일부는 그렇지 않다. 그중 하나는 전역이며 단일 네임스페이스에 묶이지 않은 노드 리소스다. 클러스터
   수준의 리소스는 4장에서...
   
  3.7.2 다른 네임스페이스와 네임스페이스의 포드
   kubectl get ns
   kubectl get namespace
   kubectl get namespaces
   
   kubectl get --namespace <namespace name>
   kubectl get -n <namespace name>
  
  3.7.3 네임스페이스 만들기
   방법1.
   디스크립터 kind: Namespace, metadata.name: <namespace name> 으로 설정 후
   kubectl create -f filename.yaml
   
   방법2.
   kubectl create namespace <namespace name>
   
  3.7.4 다른 네임스페이스의 객체 관리
   kubectl 명령어에 --namespace(또는 -n) <namespace> 을 추가하여 특정 네임스페이스의 객체를 관리할 수 있다.
  
  3.7.5 네임스페이스가 제공하는 격리
   다른 네임스페이스의 포드끼리 반드시 통신할 수 없는 것은 아니다. 네임스페이스가 네트워크 격리를 제공하는지 여부는 쿠버네티스와 함께 배포되는
   네트워킹 솔루션에 따라 다르다. 솔루션이 격리를 제공하지 않는다면 포드 IP 주소를 통해 통신이 가능하다.
   
3.8 포드의 중지와 삭제
  3.8.1 이름으로 포드 삭제
   kubectl delete po <pod name>
  
  3.8.2 라벨 셀렉터로 포드 삭제
   kubectl delete po -l <key>:<value>
  
  3.8.3 전체 네임스페이스를 삭제해 포드 삭제
   kubectl delete ns <ns name>
  
  3.8.4 네임스페이스가 유지되는 동안 모든 포드 삭제
   kubectl delete po --all
  
  3.8.5 네임스페이스의 (거의) 모든 리소스 삭제
   kubectl delete all --all : 이때 all 은 모든 유형, --all 은 이름으로 지정하는 대신 모든 리소스 인스턴스를 삭제하도록 지정함
  

4. 레플리케이션과 그 밖의 컨트롤러: 포드 배포 관리
 실제 환경에서는 배포한 애플리케이션이 자동으로 로드해 실행되고 수동으로 개입하지 않아도 안정적인 상태를 유지하길 원할 것이다. 이렇게 하기 위해
 직접 포드를 만드는 일은 거의 없다. 그 대신 레플리케이션컨트롤러 또는 디플로이먼트와 같은 유형의 리소스를 생성해 실제 포드를 만들고 관리하도록
 맡겨둔다.
 
 4.1 포드를 안정적으로 유지하기
  포드가 노드에 스케줄되자마자 해당 노드의 Kubelet은 컨테이너를 실행하고 포드가 존재하는 한 계속 실행한다. 컨테이너의 주 프로세스에 크래시가
  발생하면 kubelet이 컨테이너를 다시 시작한다. 버그 발생 시 쿠버네티스가 자동으로 다시 시작하므로 특별한 작없 없이 자동 복구가 가능하다.
  모든 문제를 해결하지는 못한다. 애플리케이션이 무한 루프 또는 교착 상태에 빠져 응답을 멈추는 상황은 애플리케이션이 다시 시작할 수 있도록
  상태를 외부에서 체크하고 내부에서 수행하는 애플리케이션에 의존하지 않아야 한다.
  
  4.1.1 라이브니스 프로브 소개
   쿠버네티스는 라이브니스 프로브(liveness probe)를 통해 컨테이너가 살이 있는지 확인할 수 있다.
   쿠버네티스는 세 가지 메커니즘 중 하나를 사용해 컨테이너를 검색한다.
    1. HTTP GET 프로브는 지정한 IP 주소, 포트, 경로에 HTTP GET 요청을 수행한다.
    2. TCP 소켓 프로브가 컨테이너의 지정된 포트에 TCP를 연결하려고 시도한다.
    3. Exec 프로브는 컨테이너 내부에 임의의 명령을 실행하고 명령의 종료 상태 코드를 확인한다. 상태 코드가 0이면 성공한 것이다.
    
  4.1.2 HTTP 기반 라이브니스 프로브 생성
   pod 디스크립터의 spec.containers에 
    livenessProbe:
     httpGet:
      path: /
      port: 8080
   방식으로 추가한다.
   이런 요청은 컨테이너가 실행되는 즉시 시작된다.
   
  4.1.3 동작 중인 라이브니스 프로브 보기
   kubectl describe po <pod name>
   > describe 를 통해 Last State란에서 Exit Code/Reason, Events란에서 이유, Liveness란에서 라이브니스 프로브의 추가 정보 등을 볼 수 있다.
   ( *컨테이너가 종료되면 완전히 새로운 컨테이너가 생성된다. 컨테이너가 재시작되는 것이 아니다.)
   
   ( *비정상정인 컨테이너의 애플리케이션 로그 얻기: --previous 옵션을 사용
     ex. kubectl logs <pod name> --previous )
     
  4.1.4 라이브니스 프로브의 추가 속성 구성
   초기 지연을 설정하지 않으면 프로브가 시작되는 즉시 컨테이너를 프로브하기 시작한다. 애플리케이션이 요청을 받을 준비가 되지 않았기 때문에
   일반적으로 프로브가 샐패한다.
   
   초기 지연되는 라이브니스 프로브
   디스크립터의 livenessProbe의 아래에 initialDelaySeconds: <num> 추가
   ( *Exit Code 137은 프로세스가 외부 신호에 의해 종료됐음을 알려준다.(종료 코드는 128 + 9(SIGKILL)). 이와 마찬가지로 종료 코드 143은 
     128 + 15(SIGTERM)에 해당한다.)
  
  4.1.5 효과적인 라이브니스 프로브 생성
   1. 라이브니스 프로브가 해야할 것
    운영환경에서 실행 중인 포드의 경우, 항상 라이브니스 프로브를 정의해야 한다. 정의되지 않으면 쿠버네티스는 애플리케이션이 아직 살아 있는지
   여부를 알 수 없다. 프로세스가 실행되는 동안 쿠버네티스는 컨테이너가 정상이라고 생각한다.
    간단한 라이브니스 프로브는 단순히 서버가 응답하는지 검사한다. 더 나은 라이브니스 프로브를 위해 특정 URL 요청을 수행하도록 프로브를 구성하고
   앱에서 실행하는 모든 중요한 구성 요소의 내부 상태를 점검해 종료됐거나 응답이 없는지 확인한다.
    이 때, 애플리케이션의 내부만 체크하고 외부 요소의 영향을 받지 않도록 하라. 예를 들어 웹서버가 백엔드 데이터베이스에 연결할 수 없을 때 실패를
   반환해서는 안된다.
   
   2. probe light 표시
    라이브니스 프로브는 너무 많은 계산 리소스를 사용해서는 안 되며 완료하는 데 너무 오랜 시간이 걸리지 않아야 한다. 기본적으로 프로브는 비교적
   자주 실행되며 1초면 완료할 수 있다. 프로브의 CPU 시간은 컨테이너의 CPU 시간 할당량을 소모하므로 CPU 시간을 꽤 소모하는 프로브가 있으면
   주 애플리케이션 프로세스에서 사용할 수 있는 CPU 시간이 줄어든다.
    ( * 컨테이너에서 자바 애플리케이션을 실행하는 경우에는 Exec 프로브 대신 HTTP GET 라이브니스 프로브를 사용해야 한다. 여기에서 liveness 
        정보를 얻기 위해 완전히 새로운 JVM을 가동한다. 시작 절차에 상당한 계산 리소스가 필요한 JVM 기반 애플리케이션이나 유사한 애플리케이션의
        경우에도 마찬가지다.)
   
   3. 프로브에서 반복 루프를 구현하지 마라
    프로브의 실패 임곗값을 구성할 수 있으며 컨테이너가 강제종료되기 전에 프로브가 여러 번 실패해야 한다. 그러나 실패 임곗값을 1로 설정하더라도
    쿠버네티스는 단 한 번의 요청으로 실패라고 판단하기 전에 프로브를 여러 번 다시 시도한다. 따라서 반복 루프를 프로브에 구현할 필요가 없다.
   
   4. 라이브니스 프로브 정리
    쿠버네티스는 크래시가 발생하거나 라이브니스 프로브가 실패한 경우 컨테이너를 다시 시작해 컨테이너를 계속 샐행한다. 이 작업은 포드를 호스팅하는
    노드에서 Kubelet에 의해 수행된다. 마스터에서 실행 중인 쿠버네티스 컨트롤 플레인 구성 요소는 이 프로세스에 포함되지 않는다.
     노드 자체에 크래시가 발생하는 경우, 노드와 함께 내려간 모든 포드의 대체품을 생성해야 하는 것은 컨트롤 플레인이다. 직접 생성한 포드는 그렇지
    않다. 이런 포드는 Kubelet을 제외하고 어떤 것으로도 관리되지 않지만, Kubelet이 노드 자체에서 실행되기 때문에 노드가 실패하면 아무것도 할 수 
    없다.

 4.2 레플리케이션컨트롤러 소개
  레플리케이션컨트롤러는 포드가 항상 실행되도록 유지하는 쿠버네티스 리소스다. 노드가 클러스터에서 사라지는 경우나 노드에서 포드가 제거된 경우와
  같이 어떤 이유로든 포드가 사라지면 해당 포드를 감지하고 대체 포드를 만든다. 레플리케이션컨트롤러는 단일 포드만 관리하지만 일반적으로
  레플리케이션컨트롤러는 포드의 여러 복사본을 만들고 관리할 수 있다.
  
  4.2.1 레플리케이션컨트롤러의 동작
   실행 중인 포드의 목록을 지속적으로 모니터링하고 '유형'의 실제 포드 수가 원하는 수와 항상 일치하는지 확인한다. 
    > 너무 적게 실행되면 포드 템플릿에서 새 복제본이 생성
    > 너무 많은 포드가 실행 중인 경우 여분의 복제본이 제거
       1. 수동으로 동일한 유형의 포드를 만든다.
       2. 누군가가 기존 포드의 '유형'을 변경한다.
       3. 누군가 원하는 포드 수를 줄였다.
       4. 기타
       
   레플리케이션컨트롤러의 역할은 정확한 수의 포드가 항상 라벨 셀렉터와 일치하는지 확인하는 것이다. 
   [160p 그림]
   
   레플리케이션컨트롤러의 세 가지 필수요소
    1. 레플리케이션컨트롤러 범위에 있는 포드를 결정하는 라벨 셀렉터
    2. 실행해야 하는 포드의 원하는 수를 지정하는 복제본 수
    3. 새로운 포드 복제본을 만들 때 사용되는 포드 템플릿
     > 3가지 모두 언제든지 수정할 수 잇지만 복제 수의 변경만 기존 포드에 영향을 미친다.
   
   컨트롤러의 라벨 셀렉터 또는 포드 템플릿 변경에 따른 효과
    라벨 셀렉터 및 포드 템플릿의 변경 사항은 기존 포드에 영향을 미치지 않는다. 라벨 셀렉터를 변경하면 기존 포드가 범위를 벗어나 컨트롤러가 해당
    포드에 신경을 쓰지 않는다. 또한 레플리케이션컨트롤러는 포드를 만든 후 컨테이너의 실제 '콘텐츠'에 신경 쓰지 않는다. 따라서 이 템플릿은 
    레플리케이션컨트롤러로 만든 새로운 포드에만 영향을 미친다. 새로운 포드를 잘라내기 위한 빵틀이라고 생각할 수 있다.
    
   레플리케이션컨트롤러의 이점
    1. 포드가 없는 경우, 새 포드를 시작해 포드가 항상 실행되도록 한다.
    2. 클러스터 노드에 장애가 발생하면 장애가 발생한 노드에서 실행 중인 모든 포드의 대체 복제본(복제 제어기의 제어 하에 있던)을 만든다.
    3. 수동으로 또는 자동으로 포드를 쉡게 수평 스케일링할 수 있다.
   
  4.2.2 레플리케이션컨트롤러 생성
   디스크립션 예시
    kind: ReplicationController
    metadata:
     name: kubia
    spec:
     replicas: 3
     selector:
      app: kubia
     telplate:
      metadata:
       labels:
        app: kubia
      spec:
       containers:
   이 컨트롤러는 항상 라벨 셀렉터 app=kubia와 일치하는 세 개의 포드 인스턴스를 항상 유지한다. 템플릿의 내용은 앞선 포드 정의와 거의 동일하다.
   템플릿의 포드 라벨은 분명히 레플리케이션 컨트롤러의 라벨 셀렉터와 일치해야 한다. 그렇지 않으면 컨트롤러는 새로운 포드를 무기한 생성한다. 이를
   방지하기 위해 API 서버는 컨트롤러 정의를 검사하고, 잘못 구성된 경우 이를 수락하지 않는다.
   셀렉터는 선택사항이다. 지정하지 않는 경우 포드 템플릿의 라벨에서 자동으로 구성된다.
   ( *레플리케이션컨트롤러를 정의할 때 포드 셀렉터를 지정하지 말라. 쿠버네티스가 포드 템플릿에서 추출할 것이다. YAML을 더 짧고 단순하게 유지한다.)
   
   kubectl create -f <rc filename>
   kubectl get rc
   kubectl describe rc <rc name> : 레플리케이션컨트롤러 상세표시
   ( *삭제중인 Pod도 Pods Status.Running에 포함된다.)
   ( *포드의 삭제나 생성에 의해서가 아닌 실제 포드 수를 확인하고 레플리케이션을 관리한다.)
   ( *노드가 죽을 경우 임시 네트워크 결함이나 Kubelet의 재시작으로 인해 노드에 도달할 수 없는 경우를 위해 잠시 기다린다. 수분 간 도달할 수 없는
      경우 포드의 상태는 Unknown으로 변경되고 새 포드를 생성한다.)
  4.2.4 레플리케이션컨트롤러의 범위 안팎으로 포드 이동
   레플리케이션컨트롤러가 생성한 포드는 어떤 식으로도 레플리케이션컨트롤러와 묶이지 않는다. 포드의 라벨을 변경해 레플리케이션컨트롤러 범위에서
   제거하거나 추가할 수 있다.
   ( *포드는 metadata.ownerReferences 필드를 통해 레플리케이션컨트롤러를 참조한다.)
   
   
